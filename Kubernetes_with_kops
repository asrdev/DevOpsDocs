Please find the below steps to setup kubernetes with kops:
Reference URL: https://jee-appy.blogspot.com/2017/10/setup-kubernetes-cluster-kops-aws.html

Pre-requisites:
- Ubuntu machines with AWS CLI installed.
  Follow this step to install aws-cli using python-pip:
     apt update
     apt ugrade
     apt install python-pip
     pip --version
     pip install awscli
     pip install --upgrade pip
     aws --version
     
     Note: Create a group called kubernetes and add a user which has following access.
           AmazonEC2FullAccess
           AmazonRoute53FullAccess
           AmazonS3FullAccess
           IAMFullAccess
           AmazonVPCFullAccess
           
     aws configure  --> AWS ACCESS_KEY & SECRET_KEY is required to configure(Go to IAM User and download the keys)
     
- curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
  chmod +x ./kubectl
  sudo mv ./kubectl /usr/local/bin/kubectl
  
- curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
  chmod +x kops-linux-amd64
  sudo mv kops-linux-amd64 /usr/local/bin/kops
  
Create Route53 domain for the cluster
  -Below can be performed only after adding a user into a group under IAM with AWDAdministerAccess && AmazonRoute53FullAccess.
  -kubernetes uses DNS for discovery inside the cluster so that you can reach out kubernetes API server from clients.
    create a hosted zone on Route53, say, manic.com The API server endpoint will then be api.manic.com
    Goto Route53 hosted zone--> Create Hosted Zone--> Fill Domain Name as "manic.com" with description and Type as "Private" & VPC ID/Region as default vpcID with us-east-2
    
 Create S3 Bucket to store all kubernetes state transaction store, so that k8s can fetch the details and do deployment accordingly.
   - From ubuntu machine where aws-cli installed.
      - aws s3 mb s3://clusters.manic.com
      -Navigate to S3 bucket and check the creation of bucket.
      
      Expose environment variable:
      $ export KOPS_STATE_STORE=s3://clusters.manic.com
      $ echo $KOPS_STATE_STORE
 
 Note: Below kops command may throw error if ssh public keys are not already generated. find below steps.
   -In a command prompt, run:  ssh-keygen -t rsa -C "your_email@example.com"
    Enter for para phrase and keep them default. You are done.
    
   - In a command prompt, run:  kops create secret --name clusters.manic.com --state s3://clusters.manic.com sshpublickey admin -i ~/.ssh/id_rsa.pub
     Note: If the above command throw error, pls check the cluster name and proceed with further command below.
     
 Create Kubernetes Cluster
   -Now we’re ready to create a cluster. You can reuse existing VPC (kops will create a new subnet in this VPC) by providing the vpc-id option:
   $ kops create cluster --cloud=aws --zones=us-east-2b --name=useast2.manic.com --dns-zone=manic.com --dns private
   
   
    
    
To actually create cluster run:
  $ kops update cluster useast2.manic.com --yes 
 
  Try after master and slave nodes are initialized.
  $ kops validate cluster
  
--->This will do all the required stuff of creating the VPC, subnets, autoscaling-groups, nodes etc. which you can observe in the output. If you want to review what all things going to happen when this command would be run then run the above command without --yes option. Without --yes option, it will print the action it is going to perform without actually doing it.
--You can then edit the cluster settings with one of these commands: 
   -List clusters with: kops get cluster
   -Edit this cluster with: kops edit cluster useast2.manic.com
   -Edit your node instance group: kops edit ig --name=useast2.manic.com nodes
   -Edit your master instance group: kops edit ig --name=useast2.manic.com master-us-east-2c
    
Then wait, it takes quite some time for the instances to boot and the DNS entries to be added in the zone. Once everything is up you should be able to get the kubernetes nodes:

   $ kubectl get nodes
   NAME                           STATUS AGE  VERSION
    ip-172-20-33-144.ec2.internal Ready  4m   v1.9
    ip-172-20-39-78.ec2.internal  Ready  1m   v1.9
    ip-172-20-45-174.ec2.internal Ready  2m   v1.9
    
Deploying the Dashboard UI
The Dashboard UI is not deployed by default. To deploy it, run the following command:

kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml   
 
Accessing the Dashboard UI
There are multiple ways you can access the Dashboard UI; either by using the kubectl command-line interface, or by accessing the Kubernetes master apiserver using your web browser.

Command line proxy
You can access Dashboard using the kubectl command-line tool by running the following command:

kubectl proxy 
Then you can use the kubctl proxy to access the UI from your machine where the command is fired or follow below:
$ kubectl proxy --port=8080 &   

Kubectl will handle authentication with apiserver and make Dashboard available at http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/.

The UI can only be accessed from the machine where the command is executed. See kubectl proxy --help for more options.

Master server:
You may access the UI directly via the Kubernetes master apiserver. Open a browser and navigate to 
https://EC2 machine IP/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/, where <master-ip> is IP address or domain name of the Kubernetes master.

Please note, this works only if the apiserver is set up to allow authentication with username and password. This is not currently the case with some setup tools (e.g., kubeadm). Refer to the authentication admin documentation for information on how to configure authentication manually.

If the username and password are configured but unknown to you, then use 
  kubectl config view  (to find it.)
 
 User will be taken to configview window to login either with token or config---> Select token and copy the token from config view of above command.

#Post validation of cluster check cluster info:
kubectl cluster-info

#You may access the UI directly via the Kubernetes master apiserver. Above kubectl cluster-info commands display kubernetes master url as below.
#Add ui at the end of this url: https://api-manic-k8s-local-cn06k4-1019785470.us-east-2.elb.amazonaws.com/ui
# To login use the command: username:admin and fire this command for token-->  
  $ kops get secrets kube --type secret -oplaintext 
#Page redirects to kubernetes asking for token---> fire this command for token--> 
  $ kops get secrets admin --type secret -oplaintext 

Welcome view
When you access Dashboard on an empty cluster, you’ll see the welcome page. This page contains a link to this document as well as a button to deploy your first application. In addition, you can view which system applications are running by default in the kube-system namespace of your cluster, for example the Dashboard itself.

How to create service accounts:

 $ kubectl create serviceaccount <admin>  ---> in the place of admin you can create developer, deployer,security accounts.
 
 $ kubectl get serviceaccounts admin -o yaml

Output of above command:

  apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-08-07T09:38:29Z
    name: admin
    namespace: default
    resourceVersion: "97414"
    selfLink: /api/v1/namespaces/default/serviceaccounts/admin
    uid: a46a2404-9a25-11e8-b450-06a11a657d12
 secrets:
 - name: admin-token-95zbk
 
Copy the name of the secret and fire command as below and get the token for the same. 
$ kubectl get secret admin-token-95zbk -o yaml 
--------------------------------------------------------------------------------------------------------
ISTIO-1.0 setup within kubernetes cluster:


Download ref link: https://istio.io/docs/setup/kubernetes/download-release/
  curl -L https://git.io/getLatestIstio | sh -

setup istioctl--> cd istio-1.0.0/bin
               chmod +x istioctl
               back to home folder(exit istio-1.0.0 folder)
               mv ./istio-1.0.0 /usr/local/bin/
               istioctl version

Below steps are to configure istio on kubernetes for AWS using kops:

Open the configuration file:

Command to check cluster: echo ${KOPS_CLUSTER_NAME}

$ kops edit cluster $YOURCLUSTER

Add the following in the configuration file:

kubeAPIServer:
    admissionControl:
    - NamespaceLifecycle
    - LimitRanger
    - ServiceAccount
    - PersistentVolumeLabel
    - DefaultStorageClass
    - DefaultTolerationSeconds
    - MutatingAdmissionWebhook
    - ValidatingAdmissionWebhook
    - ResourceQuota
    - NodeRestriction
    - Priority

Perform the update:

$ kops update cluster
$ kops update cluster --yes

Launch the rolling update:

$ kops rolling-update cluster
$ kops rolling-update cluster --yes

Validate the update with the kubectl client on the kube-api pod, you should see new admission controller:

$ for i in `kubectl \
  get pods -nkube-system | grep api | awk '{print $1}'` ; \
  do  kubectl describe pods -nkube-system \
  $i | grep "/usr/local/bin/kube-apiserver"  ; done

Review the output:

[...]
--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,
PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,
MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,
NodeRestriction,Priority
[...]

Label the default namespace with istio-injection=enabled:
   kubectl label namespace default istio-injection=enabled
   kubectl get namespace -L istio-injection

 $ kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml 
 

Automatic sidecar injection
$ kubectl api-versions | grep admissionregistration
  admissionregistration.k8s.io/v1beta1
  
$ kubectl create ns istio-system
$ kubectl apply -n istio-system -f istio.yaml

Navigate to Istio folder --> cd /usr/local/bin/istio-1.0.0

To install Istio without mutual TLS authentication between sidecars:( Below command creates/configure all resources as required).

$ kubectl apply -f install/kubernetes/istio-demo.yaml
  kubectl get pods -n istio-system
  
 NAME                                        READY     STATUS      RESTARTS   AGE
grafana-6995b4fbd7-vq8x4                    1/1       Running     0          8m
istio-citadel-54f4678f86-jzhfb              1/1       Running     0          8m
istio-cleanup-secrets-l6z8w                 0/1       Completed   0          8m
istio-egressgateway-5d7f8fcc7b-hs4bn        1/1       Running     0          8m
istio-galley-7bd8b5f88f-9pmbh               1/1       Running     0          8m
istio-grafana-post-install-ftn5b            0/1       Completed   0          8m
istio-ingressgateway-6f58fdc8d7-fw5xh       1/1       Running     0          8m
istio-pilot-d99689994-6dqm6                 2/2       Running     0          8m
istio-policy-766bf4bd6d-vlqq6               2/2       Running     0          8m
istio-sidecar-injector-85ccf84984-b4h22     1/1       Running     0          8m
istio-statsd-prom-bridge-55965ff9c8-kzvwz   1/1       Running     0          8m
istio-telemetry-55b6b5bbc7-nf2h9            2/2       Running     0          8m
istio-tracing-77f9f94b98-5dj4x              1/1       Running     0          8m
prometheus-7456f56c96-5dgqw                 1/1       Running     0          8m
servicegraph-684c85ffb9-v4t4v               1/1       Running     0          8m

$ kubectl get svc -n istio-system

NAME                       TYPE           CLUSTER-IP       EXTERNAL-IP        PORT(S)                                                                                                     AGE
grafana                    ClusterIP      100.65.172.77    <none>             3000/TCP                                                                                                    9m
istio-citadel              ClusterIP      100.71.183.104   <none>             8060/TCP,9093/TCP                                                                                           9m
istio-egressgateway        ClusterIP      100.68.244.196   <none>             80/TCP,443/TCP                                                                                              9m
istio-galley               ClusterIP      100.67.180.247   <none>             443/TCP,9093/TCP                                                                                            9m
istio-ingressgateway       LoadBalancer   100.70.234.163   ae01edadda483...   80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:32120/TCP,8060:31890/TCP,15030:31395/TCP,15031:30932/TCP   9m
istio-pilot                ClusterIP      100.69.13.117    <none>             15010/TCP,15011/TCP,8080/TCP,9093/TCP                                                                       9m
istio-policy               ClusterIP      100.68.214.4     <none>             9091/TCP,15004/TCP,9093/TCP                                                                                 9m
istio-sidecar-injector     ClusterIP      100.70.80.71     <none>             443/TCP                                                                                                     9m
istio-statsd-prom-bridge   ClusterIP      100.68.155.91    <none>             9102/TCP,9125/UDP                                                                                           9m
istio-telemetry            ClusterIP      100.71.25.146    <none>             9091/TCP,15004/TCP,9093/TCP,42422/TCP                                                                       9m
jaeger-agent               ClusterIP      None             <none>             5775/UDP,6831/UDP,6832/UDP                                                                                  9m
jaeger-collector           ClusterIP      100.64.249.39    <none>             14267/TCP,14268/TCP                                                                                         9m
jaeger-query               ClusterIP      100.68.203.254   <none>             16686/TCP                                                                                                   9m
prometheus                 ClusterIP      100.66.144.151   <none>             9090/TCP                                                                                                    9m
servicegraph               ClusterIP      100.70.47.155    <none>             8088/TCP                                                                                                    9m
tracing                    ClusterIP      100.66.240.236   <none>             80/TCP                                                                                                      9m
zipkin                     ClusterIP      100.67.25.82     <none>             9411/TCP                                                                                                    9m

Deploy Bookinfo application: https://istio.io/docs/examples/bookinfo/#if-you-are-running-on-kubernetes

If you are running on Kubernetes:
-Change directory to the root of the Istio installation directory.

-Bring up the application containers:

-If you are using manual sidecar injection, use the following command

  $ kubectl apply -f <(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)

The istioctl kube-inject command is used to manually modify the bookinfo.yaml file before creating the deployments as documented here.

-If you are using a cluster with automatic sidecar injection enabled, label the default namespace with istio-injection=enabled

  $ kubectl label namespace default istio-injection=enabled

-Then simply deploy the services using kubectl

  $ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
Confirm all services and pods are correctly defined and running:

$ kubectl get services
NAME                       CLUSTER-IP   EXTERNAL-IP   PORT(S)              AGE
details                    10.0.0.31    <none>        9080/TCP             6m
kubernetes                 10.0.0.1     <none>        443/TCP              7d
productpage                10.0.0.120   <none>        9080/TCP             6m
ratings                    10.0.0.15    <none>        9080/TCP             6m
reviews                    10.0.0.170   <none>        9080/TCP             6m

$ kubectl get pods
NAME                                        READY     STATUS    RESTARTS   AGE
details-v1-1520924117-48z17                 2/2       Running   0          6m
productpage-v1-560495357-jk1lz              2/2       Running   0          6m
ratings-v1-734492171-rnr5l                  2/2       Running   0          6m
reviews-v1-874083890-f0qf0                  2/2       Running   0          6m
reviews-v2-1343845940-b34q5                 2/2       Running   0          6m
reviews-v3-1813607990-8ch52                 2/2       Running   0          6m

Determining the ingress IP and port:
Define the ingress gateway for the application:
  $ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml

Confirm the gateway has been created:
  $ kubectl get gateway
NAME               AGE
bookinfo-gateway   32s

Follow these instructions(https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports)
to set the INGRESS_HOST and INGRESS_PORT variables for accessing the gateway. Return here, when they are set.

Determining the ingress IP and ports
Execute the following command to determine if your Kubernetes cluster is running in an environment that supports external load balancers:

$ kubectl get svc istio-ingressgateway -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                                      AGE
istio-ingressgateway   LoadBalancer   172.21.109.129   130.211.10.121  80:31380/TCP,443:31390/TCP,31400:31400/TCP   17h

If the EXTERNAL-IP value is set, your environment has an external load balancer that you can use for the ingress gateway. If the EXTERNAL-IP value is <none> (or perpetually <pending>), your environment does not provide an external load balancer for the ingress gateway. In this case, you can access the gateway using the service’s node port.

Depending on your environment, follow the instructions in one of the following mutually exclusive subsections.

Determining the ingress IP and ports when using an external load balancer
Follow these instructions if you have determined that your environment does have an external load balancer.

Set the ingress IP and ports:

$ export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
$ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
$ export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')

Note that in certain environments, the load balancer may be exposed using a host name, instead of an IP address. In this case, the EXTERNAL-IP value in the output from the command in the previous section will not be an IP address, but rather a host name, and the above command will have failed to set the INGRESS_HOST environment variable. In this case, use the following command to correct the INGRESS_HOST value:

$ export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

Determining the ingress IP and ports when using a node port
Follow these instructions if you have determined that your environment does not have an external load balancer.

Set the ingress ports:

$ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}')
$ export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}')


Other environments (e.g., IBM Cloud Private etc):

$ export INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o 'jsonpath={.items[0].status.hostIP}')

Create NLB First for master node.
Configuring Istio Ingress with AWS NLB: Follow below URL...
https://istio.io/blog/2018/aws-nlb/
URL looks like this: http://ae2f8c4f99ef811e8a845068912c2798-1062715363.us-east-2.elb.amazonaws.com/productpage

To confirm that the Bookinfo application is running, run the following curl command:

$ curl -o /dev/null -s -w "%{http_code}\n" http://ae2f8c4f99ef811e8a845068912c2798-1062715363.us-east-2.elb.amazonaws.com/productpage
200

Apply default destination rules:(This is required if you want to control Bookinfo version routing)

   Before you can use Istio to control the Bookinfo version routing, you need to define the available versions, called subsets, in destination rules.

  Run the following command to create default destination rules for the Bookinfo services:

  If you did not enable mutual TLS, execute this command:

    $ kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml

  If you did enable mutual TLS, execute this command:

   $ kubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml

Wait a few seconds for the destination rules to propagate.

  You can display the destination rules with the following command:

   $ kubectl get destinationrules -o yaml
--------------------------------------------------------------------------------------------------------
      
Deploying Nginx Container
To test our new Kubernetes cluster, we could deploy a simple service made up of some nginx containers:
Create an nginx deployment:

$ kubectl run sample-nginx --image=nginx --replicas=2 --port=80
   Result: deployment.apps/sample-nginx created
   
$ kubectl get pods

NAME                       READY     STATUS    RESTARTS   AGE
sample-nginx-379829228-xb9y3   1/1       Running   0          10s
sample-nginx-379829228-yhd25   1/1       Running   0          10s

$ kubectl get deployments

NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
sample-nginx   2         2         2            2           29s

Expose the deployment as service. This will create an ELB in front of those 2 containers and allow us to publicly access them:
 
$ kubectl expose deployment sample-nginx --port=80 --type=LoadBalancer

$ kubectl get services -o wide

NAME         CLUSTER-IP      EXTERNAL-IP                                                              PORT(S)   AGE       SELECTOR
kubernetes   100.64.0.1      <none>                                                                   443/TCP   25m       <none>
sample-nginx     100.70.129.69   adca6650a60e611e7a66612ae64874d4-175711331.us-east-1.elb.amazonaws.com/   80/TCP    19m       run=sample-nginx
 
There is an ELB running on http://adca6650a60e611e7a66612ae64874d4-175711331.us-east-1.elb.amazonaws.com with our nginx containers behind it:   

You can also view the UI by accessing master node. Hit master node's IP/Domain in browser, it will ask for credentials. 

Run command kubectl config view to see the credentials.
    
    
    
    
    
  

